{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLHcLz5zvGMH"
      },
      "source": [
        "# NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
        "\n",
        "In this project we will be teaching a neural network to translate from German to English.\n",
        "\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence\n",
        "to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
        "learn to focus over a specific range of the input sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs-8uyBGvGMP"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ttoEcLnfvGMR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%%capture` not found.\n"
          ]
        }
      ],
      "source": [
        "# using Python 3.9\n",
        "%%capture\n",
        "%pip install pandas torch matplotlib numpy ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i381hyrevGMT"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import os\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULBf_g5pvGMV"
      },
      "source": [
        "## Loading & preprocessing training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3FBrubJvGMd"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation. We then implement a parsing funciton that reads a csv file from the specified path, normalizes every sentence in the dataset using the normalize function, and returns a list of (german_sentence, english_sentence) pairs. If the parameter reverse is True, the order of languages in the tuple are reversed resulting in reverse translation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q_GJNRPYvGMg"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        'ss' if c == 'ß' else c # account for eszett\n",
        "        for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# lowercase, trim, and remove non-letter characters\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def parse_data(path, reverse=False):\n",
        "\n",
        "  pairs = pd.read_csv(path)\n",
        "  pairs['ENG'] = pairs['ENG'].apply(normalize_string)\n",
        "  pairs['GER'] = pairs['GER'].apply(normalize_string)\n",
        "\n",
        "  if reverse:\n",
        "    pairs = pairs[['GER', 'ENG']]\n",
        "\n",
        "  return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eHoLTeevGMk"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences and we will avoid explicit questions. Hence we will set a maximum sentence length of 10 words (that includes punctuation) and remove questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sSAya9DovGMk"
      },
      "outputs": [],
      "source": [
        "MAX_WORDS = 10\n",
        "\n",
        "interrogative_words = ( \"what\", \"why\", \"how\", \"which\", \"where\", \"when\" , \"who\", \"whose\",\n",
        "                        \"was\", \"warum\", \"wie\", \"welche\", \"wo\", \"wann\", \"wer\", \"wieso\")\n",
        "\n",
        "\"\"\"\n",
        "TODO: Task 1 (10 pt)\n",
        "\n",
        "Implement the filter_pairs function so that it takes in a pd.DataFrame of language pairs and outputs another dataframe where following pairs are removed:\n",
        "\n",
        "- those where one of both exceeds the maximal sentence length (MAX_WORDS)\n",
        "- those where the pair contains any of the words defined in interrogative_words or a question mark.\n",
        "\n",
        "Note: your method should NOT employ any explicit iteration (i.e. for loops).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def filter_pairs(pairs,\n",
        "                 drop_words=interrogative_words,\n",
        "                 max_words=MAX_WORDS):\n",
        "\n",
        "    pairs = pairs[\n",
        "        (pairs['ENG'].str.split().str.len() < max_words) & # account for zero indexing\n",
        "        (pairs['GER'].str.split().str.len() < max_words)\n",
        "    ]\n",
        "\n",
        "    # creating a regex pattern for interrogative words and question marks\n",
        "    question_pattern = fr\"\\b(?:{'|'.join(drop_words)})\\b|\\?\"\n",
        "\n",
        "    # filter based on interrogative words and question marks\n",
        "    pairs = pairs[\n",
        "        ~pairs['ENG'].str.contains(question_pattern, case=False, regex=True) &\n",
        "        ~pairs['GER'].str.contains(question_pattern, case=False, regex=True)\n",
        "    ]\n",
        "\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ou8g3IGvGMX"
      },
      "source": [
        "Each word in a language will be represented as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfCoiIMHvGMY"
      },
      "source": [
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Language`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` which will be used to replace rare words later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yfuEobYvvGMb"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\"\"\"\n",
        "TODO: Task 2 (10 pt)\n",
        "\n",
        "Implement the function remove_rare_words, that removes words whose frequency is below\n",
        "min_freq and makes sure that the size of the vocabulary is below max_vocab_size\n",
        "by iteratively removing least frequent words.\n",
        "\n",
        "The function should also update the indices in the dictionary, so that no index\n",
        "is left unused (i.e. all values from 0 to n_words are indices) and all variables are up-to-date.\n",
        "\n",
        "You might want to define a helper method remove_word to break down the task.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Language:\n",
        "    def __init__(self):\n",
        "        self.word2index = {} # maps word to integer index\n",
        "        self.word2count = {} # maps word to its frequency\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # maps index to a word\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def remove_word(self, word):\n",
        "      if word in self.word2index:\n",
        "        index = self.word2index[word]\n",
        "        del self.index2word[index] # remove word from integer to word index\n",
        "        del self.word2index[word] # remove word from word to integer index\n",
        "        del self.word2count[word] # remove word from frequency count\n",
        "\n",
        "        # update word to integer index for indices that come after removed word\n",
        "        for w, i in list(self.word2index.items()):\n",
        "            if i > index:\n",
        "                self.word2index[w] -= 1\n",
        "\n",
        "        # update integer to word index for words that come after removed word\n",
        "        for i in range(index + 1, self.n_words):\n",
        "            self.index2word[i - 1] = self.index2word[i]\n",
        "\n",
        "        # remove the last entry of integer to word index as it is now a duplicate\n",
        "        del self.index2word[self.n_words - 1]\n",
        "\n",
        "        # update number of words\n",
        "        self.n_words -= 1\n",
        "\n",
        "    def remove_rare_words(self, min_freq, max_vocab_size):\n",
        "      words_to_remove = [word for word in self.word2count if self.word2count[word] < min_freq] # list of words to remove\n",
        "      words_to_remove.sort(key=lambda word: self.word2count[word]) # sort words by frequency\n",
        "\n",
        "      # remove words until the vocabulary size is below max_vocab_size\n",
        "      while self.n_words > max_vocab_size:\n",
        "        if not words_to_remove:\n",
        "          break\n",
        "        word_to_remove = words_to_remove.pop(0)\n",
        "        self.remove_word(word_to_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGhDql9vGMl"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O5LZIJ9yvGMl"
      },
      "outputs": [],
      "source": [
        "def prepare_data(pairs, reverse=False):\n",
        "\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "    pairs = filter_pairs(pairs)\n",
        "    print(f\"Filtered {len(pairs)} sentence pairs\")\n",
        "    pairs = pairs.to_numpy()\n",
        "\n",
        "    input_lang = Language()\n",
        "    output_lang = Language()\n",
        "\n",
        "    for pair in pairs:\n",
        "        input_lang.add_sentence(pair[0])\n",
        "        output_lang.add_sentence(pair[1])\n",
        "\n",
        "    print(f\"Input language: {input_lang.n_words} words\")\n",
        "    print(f\"Output language: {output_lang.n_words} words\")\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmKUd4oRbf3n",
        "outputId": "5fd2b7d6-fbc6-4968-84f8-71ac30e61c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 255817 sentence pairs\n",
            "Filtered 145581 sentence pairs\n",
            "Input language: 12982 words\n",
            "Output language: 25355 words\n"
          ]
        }
      ],
      "source": [
        "#path = os.path.join(os.getcwd(), \"data\", \"translations.csv\")\n",
        "path = \"https://raw.githubusercontent.com/smkerr/problem-set-3-ps3_group_b/main/data/translations.csv\"\n",
        "pairs = parse_data(path)\n",
        "input_lang, output_lang, pairs = prepare_data(pairs, reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWT9l3A0vGMl"
      },
      "source": [
        "## Encoder-decoders\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215)_, or\n",
        "seq2seq network, or [Encoder Decoder\n",
        "network](https://arxiv.org/pdf/1406.1078v3.pdf)_, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages, because there usally is not a 1:1 mapping between words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQdvPf6cvGMn"
      },
      "source": [
        "### The Encoder\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "Yo1C0Gw8vGMn",
        "outputId": "2528c23b-fa98-4791-bc6f-175042e0436d"
      },
      "outputs": [],
      "source": [
        "Image(filename=\"img/encoder.png\", height=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "UKk5mcDYvGMn"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, rnn_type='GRU', batch_size=1, device=device):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "        self.rnn_type = rnn_type\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size) #produces a list of vect\n",
        "        self.rnn = getattr(nn, rnn_type)(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1,1,-1) # translates input (series of indices for the respective words) into their vector representation\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.batch_size, self.hidden_size, device=self.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmcFTutVDVM"
      },
      "source": [
        "Although all encoders must have certain distinctive features to be called as such, as for example the ability to process sequences of different length returning fixed-length hidden states, it is not required for them to have a recurrent structure. In fact, most state-of-the art NLP models use transformer layers, combining attention layers with feed-forward network for a higher parallelizability. Because transformer layers process information in parallel, they have no inherent mechanisms that makes them aware of the position of the word in the sentence and need to engineer a feature that encodes that information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "SbA-XuPSN3QQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TODO: Task 3 (10 pt)\n",
        "\n",
        "Implement a positional encoding module for an alternative encoder using transformers.\n",
        "The positional encoder should create a vector of size max_length containing the\n",
        "discrete values of a function f(x) applied on the even features of the input\n",
        "and g(x) applied on the odd features of the input.\n",
        "f(x) should be defaulted to sine, g(x) to cosine.\n",
        "Their value should be scaled by div_term.\n",
        "\n",
        "Briefly describe (3-4 sentences):\n",
        "\n",
        "- why sinusoidal are used by default\n",
        "- how div_term impacts the encoding\n",
        "- which characteristics would make a function a suitable\n",
        "  alternative to sinusoidals.\n",
        "\n",
        "Note: you are not expected to train the TransformerEncoder (this will most likely\n",
        "not work due to incompatibility with the train method). Simply show you can initialize\n",
        "the encoder and pass a tensor through it.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_size,\n",
        "                 max_length,\n",
        "                 even_fc=torch.sin,\n",
        "                 odd_fc=torch.cos):\n",
        "\n",
        "        super().__init__()\n",
        "        div_term = 1 / (torch.exp(torch.arange(0, hidden_size, 2, dtype=torch.float) *\n",
        "                             (math.log(10000.0) / hidden_size)))\n",
        "\n",
        "        # positional encodings\n",
        "        self.positional_encodings = torch.zeros(max_length, hidden_size) # max_length x hidden_size ensure that the positional encodings have the same size as the input\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1) # max_length x 1 vector containing the position of each word in the input\n",
        "        self.positional_encodings[:, 0::2] = even_fc(position * div_term) # apply even function to even features in order to create the positional encodings\n",
        "        self.positional_encodings[:, 1::2] = odd_fc(position * div_term) # apply odd function to odd features in order to create the positional encodings\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"In the forward step, the positional_encoder vector should be added to the input.\"\"\"\n",
        "        return input + self.positional_encodings\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, max_length=MAX_WORDS, batch_size=1):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_size, max_length)\n",
        "        self.transformer_layers = nn.TransformerEncoderLayer(hidden_size, nhead=4)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layers, num_layers=6)\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, positional_encoding=False):\n",
        "\n",
        "        embedded = self.embedding(input) # translates input (series of indices for the respective words) into their vector representation\n",
        "\n",
        "        # If true, applies positional encoding\n",
        "        if positional_encoding:\n",
        "            embedded = self.positional_encoding(embedded).view(1, 1, -1)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        transformer_output = self.transformer_encoder(embedded)\n",
        "\n",
        "        # Fully connected layer\n",
        "        output = self.fc(transformer_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    # defining tensor from sentence method to make the encoder compatible with the decoder\n",
        "    def tensorFromSentence(self, sentence):\n",
        "        indexes = [input_lang.word2index[word] for word in sentence.split(' ')]\n",
        "        indexes.append(EOS_token)\n",
        "        return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vI7SvcEybf3o"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-4.1769e-01,  5.0893e-01, -9.5441e-02,  2.0867e-01, -5.9357e-01,\n",
              "           5.2481e-02, -2.1749e-01, -9.2590e-02, -6.3563e-01,  5.6829e-01,\n",
              "           6.2869e-01,  2.1348e-01, -1.7159e-01, -1.2895e+00, -5.7395e-01,\n",
              "           6.7668e-01, -6.5501e-01, -9.0820e-01, -7.2859e-01,  2.7913e-01,\n",
              "           8.4095e-01, -5.2859e-01,  3.9516e-01, -3.5808e-01, -8.4342e-01,\n",
              "          -2.3671e-01, -5.7909e-02, -9.9490e-01, -9.8250e-01,  4.1512e-02,\n",
              "          -5.7250e-01, -2.0012e-01,  3.0208e-01, -8.3089e-01, -1.4534e-01,\n",
              "          -1.4406e-01, -8.1816e-01, -3.3774e-01,  2.6550e-01, -1.0127e-01,\n",
              "          -6.6079e-01,  5.6895e-01,  8.8999e-01,  1.8085e-02, -8.0678e-01,\n",
              "          -1.1518e-02,  1.0765e-01,  8.4229e-01, -2.8150e-01, -5.1609e-01,\n",
              "          -4.0990e-02, -1.3272e+00, -2.7997e-01,  7.4448e-01, -1.1613e+00,\n",
              "          -5.5500e-01, -8.9731e-02, -1.6422e-01,  8.5494e-01, -7.8579e-01,\n",
              "           1.9517e-01,  4.8700e-02,  1.0378e-01, -7.2315e-01,  1.3632e-01,\n",
              "          -3.6874e-01,  5.3487e-01,  9.7689e-04, -3.5316e-01, -3.8250e-01,\n",
              "           5.9894e-01,  4.0140e-01,  1.1296e+00,  3.5818e-01,  1.7144e-01,\n",
              "          -6.9300e-02,  9.5834e-01,  3.3823e-01, -3.1217e-02, -3.6283e-01,\n",
              "           6.1486e-01, -1.4608e-01,  4.2930e-01, -7.1882e-01, -2.1790e-01,\n",
              "          -8.5945e-01, -6.5512e-02, -1.8340e-01,  6.4782e-01, -3.7409e-01,\n",
              "           4.9429e-01,  2.6945e-01,  2.2718e-01, -2.6409e-02,  3.7461e-01,\n",
              "          -3.1923e-01,  4.2269e-01, -9.5857e-03,  5.3881e-01, -1.0103e-01,\n",
              "          -6.8721e-01, -5.1448e-01, -4.1422e-01, -2.7502e-01,  4.3493e-01,\n",
              "          -3.9295e-01, -4.6032e-02, -7.5410e-01, -9.0001e-01,  8.7843e-01,\n",
              "           1.9350e-01,  5.5122e-01, -6.1607e-02, -1.5104e-01, -5.1763e-01,\n",
              "           1.2160e-01,  8.8362e-01, -4.3022e-01,  1.4910e+00,  2.1555e-01,\n",
              "          -1.1649e+00,  2.9623e-01, -1.0222e+00, -1.4355e+00, -1.1453e-01,\n",
              "          -1.3127e-01, -1.2283e+00,  9.7635e-01,  6.0370e-01, -3.3638e-01,\n",
              "          -1.8108e-01, -4.4700e-01, -5.3830e-01, -5.7886e-01, -4.2700e-01,\n",
              "           1.4007e-01,  2.9111e-01,  4.5624e-01, -2.5464e-01, -6.6087e-01,\n",
              "           6.3756e-01,  7.1414e-01, -2.2305e-01,  2.6766e-01, -7.5775e-01,\n",
              "           1.8732e-01, -4.9982e-01,  7.0521e-01, -2.8316e-01, -5.0496e-01,\n",
              "           6.7338e-01, -4.4330e-01,  2.0206e-01,  9.1898e-01, -4.6819e-01,\n",
              "          -5.4686e-01,  7.1631e-01,  1.6444e+00, -1.0994e+00, -1.6603e-01,\n",
              "           2.4946e-01, -3.9666e-01,  4.3643e-01, -3.2978e-01,  8.2474e-01,\n",
              "           4.5007e-01, -2.7418e-01,  1.9667e-01, -2.4301e-02, -1.4809e-02,\n",
              "           2.9838e-01,  1.1927e+00,  3.4201e-01, -4.2010e-01,  8.3224e-01,\n",
              "           6.7212e-01,  8.2713e-02,  4.8303e-02, -4.5396e-01,  1.2635e-01,\n",
              "          -9.4695e-01, -7.1044e-01, -1.1251e+00,  5.4813e-01, -1.8700e-01,\n",
              "          -4.2580e-01, -1.0194e+00, -1.0460e-02, -4.4652e-01,  2.0095e-01,\n",
              "          -4.3638e-01,  1.3833e+00, -5.8973e-01, -3.4711e-02,  4.0626e-01,\n",
              "           7.0420e-01,  6.5368e-01,  1.8580e-01, -4.6582e-01,  3.9231e-01,\n",
              "          -8.3010e-02, -1.1456e+00, -1.0289e+00,  3.8094e-01, -2.1435e-02,\n",
              "           3.3227e-01, -4.9683e-01,  7.8193e-01,  8.4315e-01, -2.5276e-01,\n",
              "           2.4462e-03,  1.3290e+00,  3.4004e-01,  9.4094e-02, -1.4584e-01,\n",
              "          -1.5423e+00, -1.4470e+00,  3.4527e-01,  5.0598e-01, -2.1618e-01,\n",
              "          -2.0660e-01,  6.8621e-01, -6.1541e-01, -2.9935e-01,  6.5979e-01,\n",
              "           1.0094e+00, -2.1623e-01,  4.3946e-01,  6.6965e-01, -1.4833e-01,\n",
              "          -6.1312e-02, -6.7303e-01, -3.5724e-01,  7.7483e-01, -8.7476e-01,\n",
              "           4.2987e-01,  1.4385e-02,  1.5367e-01,  6.3502e-02, -2.3155e-01,\n",
              "           6.0020e-01, -1.3286e-01, -2.4652e-01,  4.4300e-01,  2.6407e-01,\n",
              "           7.8584e-01,  6.5281e-01,  1.3256e+00,  1.0487e-01, -5.1112e-03,\n",
              "          -4.4065e-02, -5.2215e-01,  3.8021e-01, -1.2204e-01, -7.6843e-01,\n",
              "           4.5196e-01]],\n",
              "\n",
              "        [[-6.4625e-01,  8.7009e-01,  1.4212e-01,  2.5344e-03, -2.8475e-01,\n",
              "          -1.7265e-01, -2.2347e-01,  3.6789e-02, -1.2259e+00,  2.0916e-01,\n",
              "           7.4118e-01,  6.7135e-01, -6.6324e-01, -1.0266e+00, -1.1837e+00,\n",
              "           5.3353e-01, -3.2036e-01, -9.6504e-01, -2.5004e-01,  2.9662e-01,\n",
              "           2.2160e-01, -6.1156e-01, -2.1699e-01, -8.3759e-01, -5.8896e-01,\n",
              "          -3.0504e-01,  1.2228e-01, -7.2573e-01, -1.2093e+00,  3.4888e-02,\n",
              "          -6.0166e-01, -1.7160e-01,  9.7715e-02, -6.1672e-01, -2.9733e-01,\n",
              "          -5.7273e-01, -7.4025e-01, -1.1462e+00,  3.5866e-01, -5.2650e-01,\n",
              "          -2.0058e-01,  3.0475e-01,  1.4528e+00,  5.1997e-01, -5.8607e-01,\n",
              "          -4.1353e-01,  8.1889e-01,  1.9759e-01, -6.7328e-01, -7.7401e-01,\n",
              "          -3.0048e-01, -1.1029e+00,  1.7297e-01,  1.1829e-01, -2.2404e+00,\n",
              "          -1.6506e-01,  6.5601e-01,  2.0691e-01,  4.2571e-01, -5.9162e-01,\n",
              "           7.3370e-02, -3.3062e-01,  2.8991e-01, -2.4170e-01, -4.3553e-01,\n",
              "          -8.7473e-01,  5.7958e-01,  4.5748e-02, -1.0318e-02,  2.5254e-01,\n",
              "          -2.6388e-01, -1.4890e-01,  1.1234e+00,  1.6951e-01,  4.6954e-01,\n",
              "          -1.0742e-01,  2.1722e-01,  8.1267e-02, -2.4827e-03, -5.1474e-01,\n",
              "           8.3647e-01,  1.3275e-01,  4.4267e-01, -7.9711e-01, -2.8001e-01,\n",
              "          -4.0722e-01, -9.7785e-02, -2.4771e-01, -1.2585e-01, -7.3763e-01,\n",
              "           1.2562e-02,  5.9277e-01,  3.5670e-01,  3.4056e-02, -1.7639e-01,\n",
              "           2.1684e-02,  5.4480e-01, -2.8884e-01,  3.8023e-01,  6.5646e-01,\n",
              "          -3.4248e-01, -3.0457e-01, -6.6194e-01, -5.0812e-01,  3.5729e-01,\n",
              "           2.1731e-01, -5.9236e-01, -2.6057e-01, -5.8735e-01, -6.3826e-02,\n",
              "           1.5350e-02,  5.1681e-01, -4.1963e-01, -3.6189e-01, -6.8825e-01,\n",
              "           1.0827e+00,  1.2589e+00, -5.9647e-01,  1.3537e+00, -3.7217e-01,\n",
              "          -6.6766e-01,  5.8841e-01, -7.7196e-01, -2.6566e-01,  2.1425e-01,\n",
              "           1.0190e-01, -9.1997e-01,  2.5402e-01,  8.5688e-01,  4.2751e-01,\n",
              "           3.1107e-01, -3.4228e-01, -5.5623e-01, -6.8995e-01, -6.4224e-01,\n",
              "           8.5541e-02,  3.9224e-01,  5.0528e-01, -7.9495e-01, -6.3079e-01,\n",
              "          -2.1212e-01,  1.5640e+00, -6.2209e-01,  5.9958e-01, -4.2428e-01,\n",
              "          -2.7282e-01, -6.0860e-02,  3.2842e-01, -9.8923e-01, -5.4907e-01,\n",
              "           6.8777e-01, -8.0320e-01, -1.6057e-01,  7.6055e-01, -5.3043e-01,\n",
              "          -6.5209e-01,  4.5225e-01,  1.0038e+00, -6.1966e-01,  7.5148e-02,\n",
              "           9.6463e-01, -4.7121e-01,  1.4341e-01,  3.6863e-01,  1.0726e+00,\n",
              "           8.6850e-01,  2.9774e-01, -4.7689e-01,  1.8021e-01,  3.2880e-01,\n",
              "           3.9454e-01,  1.1874e+00,  1.0699e-01,  1.6730e-01,  4.0216e-01,\n",
              "          -9.4756e-02,  2.2218e-01, -5.1985e-01, -5.0940e-01, -6.1310e-01,\n",
              "          -4.2093e-01, -6.9689e-01, -1.1504e+00,  3.1273e-01, -1.0004e+00,\n",
              "           2.3696e-01, -8.6279e-01,  7.8248e-01, -5.0874e-01,  7.2765e-01,\n",
              "          -6.8346e-01,  7.1059e-01,  3.7388e-01, -1.9280e-01,  5.9645e-01,\n",
              "           2.6596e-01,  4.3770e-01,  2.8257e-01, -9.0321e-01,  1.3274e-02,\n",
              "          -5.5018e-02, -1.2780e+00, -1.1040e+00,  2.6176e-01,  2.2290e-01,\n",
              "          -4.6708e-01, -1.4816e+00,  5.1895e-01,  1.1445e+00, -3.6189e-01,\n",
              "           4.9259e-01,  1.4807e+00,  4.5930e-01,  1.2268e-02, -5.2560e-01,\n",
              "          -1.0546e+00, -1.5882e+00,  1.4372e-01,  6.3657e-01, -1.6971e-01,\n",
              "          -4.2660e-01,  5.4218e-01, -8.6254e-02,  5.4811e-01,  2.3037e-01,\n",
              "           6.2650e-01, -6.3721e-01,  7.4993e-01, -1.5307e-01, -2.1710e-01,\n",
              "           7.5942e-02, -3.6225e-01, -2.1149e-01,  7.7442e-01, -1.3646e+00,\n",
              "          -1.9682e-01, -2.2665e-02,  3.3509e-01, -8.4152e-02, -1.3876e-02,\n",
              "           4.3632e-01, -4.7207e-01, -4.2004e-01,  6.8191e-01,  4.8459e-01,\n",
              "           4.4881e-01,  4.1587e-01,  1.0882e+00,  5.9053e-02,  4.2664e-02,\n",
              "          -3.3797e-01, -4.9235e-01,  1.0975e+00, -2.8915e-01,  2.7111e-01,\n",
              "          -2.9906e-01]],\n",
              "\n",
              "        [[-5.3206e-01,  8.0253e-01, -3.7242e-01, -1.4745e-01, -9.3402e-01,\n",
              "           1.3298e-02, -6.1374e-01, -1.3903e-01, -7.5348e-01,  8.1727e-01,\n",
              "           1.0541e+00,  3.0002e-01, -4.4338e-01, -1.2140e+00, -4.4956e-01,\n",
              "           1.4308e-01, -1.0244e+00, -1.5091e-01, -4.7068e-01,  3.7990e-01,\n",
              "          -1.8762e-02, -4.5128e-01, -4.0039e-01, -2.3140e-01, -4.7445e-01,\n",
              "          -5.7952e-01,  1.7393e-01, -8.4213e-01, -8.3434e-01, -1.7281e-01,\n",
              "          -2.2088e-01, -1.2271e-01,  1.3440e-01, -2.2856e-01, -2.4477e-01,\n",
              "           1.8137e-01, -7.9380e-01, -1.0545e+00,  2.3668e-01, -3.7641e-01,\n",
              "          -6.6926e-01, -3.6376e-03,  4.2098e-01,  7.8974e-02, -8.7420e-01,\n",
              "           2.3720e-03,  2.4356e-01,  2.6613e-01, -3.1765e-01, -6.8701e-01,\n",
              "          -5.8297e-01, -1.2829e+00, -1.2956e-01,  8.2574e-01, -2.0277e+00,\n",
              "           3.9836e-02, -1.1376e-01,  2.8550e-01,  5.0820e-01, -1.4130e-01,\n",
              "           5.3493e-02, -1.1361e-01,  3.9227e-01, -6.3919e-01, -3.6841e-01,\n",
              "          -6.7831e-02,  3.2180e-01, -1.5629e-01, -1.6737e-01, -4.5399e-01,\n",
              "          -2.2748e-01,  7.3883e-03,  9.0353e-01,  2.8647e-01,  4.6693e-01,\n",
              "          -1.6957e-01,  8.9214e-01,  2.5776e-01,  3.5111e-01, -1.7801e-01,\n",
              "           4.6125e-01, -1.5174e-01,  5.9183e-01, -1.9740e-01, -8.1018e-02,\n",
              "          -5.1220e-01, -3.4773e-01,  3.6211e-01,  3.3732e-01, -3.2653e-01,\n",
              "          -1.9984e-01,  1.3169e+00,  4.4544e-01, -8.4117e-02,  1.2216e-01,\n",
              "          -6.1452e-01,  4.2455e-01, -1.7617e-01,  3.4223e-01,  4.1606e-01,\n",
              "          -3.9196e-01,  4.3280e-01, -9.0317e-01, -6.5534e-02, -7.6894e-02,\n",
              "           4.2344e-01,  5.0134e-01, -3.6746e-01, -3.5075e-01,  6.2934e-01,\n",
              "           4.0499e-01, -4.8094e-02, -4.6609e-01, -6.0280e-01,  9.6943e-02,\n",
              "           8.2856e-01,  1.0717e+00, -4.2091e-01,  2.3004e+00, -6.1448e-01,\n",
              "          -9.2396e-01,  4.3092e-01, -9.4771e-01, -1.0787e+00, -2.9650e-03,\n",
              "          -3.5958e-01, -1.8587e-01,  3.0330e-01,  4.8251e-01,  1.5917e-01,\n",
              "           5.0149e-02, -2.4047e-01, -2.5488e-02, -4.9598e-01, -3.8709e-01,\n",
              "           1.9835e-01,  8.0049e-02,  1.0913e-01, -5.7655e-01,  2.4913e-01,\n",
              "          -2.7061e-01,  9.0095e-01, -4.1175e-02,  4.9657e-01, -1.0728e+00,\n",
              "           5.4413e-02, -1.6355e-01,  1.0289e+00,  4.4785e-01, -4.8240e-01,\n",
              "           1.2010e+00, -8.6700e-01,  1.0564e-01,  7.7883e-01, -1.3606e-01,\n",
              "          -5.6335e-01,  1.0727e+00,  6.4849e-01, -1.2530e+00, -5.6045e-02,\n",
              "           7.2401e-01, -3.3410e-02,  3.8183e-01, -2.0928e-01,  9.5601e-01,\n",
              "           6.9240e-01,  5.8993e-01, -5.1723e-01, -2.8922e-01,  5.8193e-03,\n",
              "           5.4960e-01,  5.9903e-01,  1.2733e-01, -5.8121e-01,  9.2765e-01,\n",
              "           1.9768e-01, -2.0299e-01, -1.3671e-01,  2.6361e-01, -8.3633e-01,\n",
              "          -9.1984e-01, -8.5754e-01, -1.1407e+00,  3.3197e-01,  1.0975e-01,\n",
              "          -1.5772e-01, -1.1878e+00,  4.5641e-01, -7.6909e-01,  6.0844e-01,\n",
              "          -3.4547e-01,  1.5234e+00, -2.0051e-01,  1.0341e-01,  7.0967e-01,\n",
              "           2.4097e-01,  8.4965e-01,  1.6330e-01, -3.9107e-01,  1.2204e-01,\n",
              "           1.1453e-01, -8.9350e-01, -5.8880e-01,  5.7704e-01,  1.5407e-01,\n",
              "          -4.6384e-01, -8.6645e-01,  4.7016e-01,  1.0090e+00, -9.2459e-01,\n",
              "           7.0262e-01,  1.5159e+00,  3.9485e-01,  6.0729e-01, -2.4288e-01,\n",
              "          -1.3447e+00, -1.1845e+00, -5.9674e-02,  7.0778e-01,  1.6541e-01,\n",
              "          -6.6256e-01,  3.5806e-01, -5.5793e-01,  3.9287e-01,  5.0000e-01,\n",
              "           4.9202e-01, -5.5908e-01,  2.0694e-01, -3.8190e-01, -2.4697e-01,\n",
              "          -5.8002e-01, -6.2685e-01,  2.9276e-01,  7.7525e-01, -6.4907e-01,\n",
              "           3.6519e-01,  3.7966e-01,  1.2632e-01, -4.5871e-01, -2.9614e-01,\n",
              "           1.4459e-01, -4.5217e-01, -3.2840e-01,  6.9042e-01, -3.6275e-01,\n",
              "           2.7345e-02,  3.9841e-01,  1.0996e+00, -3.1780e-01,  5.2149e-01,\n",
              "           3.1253e-01, -6.9353e-01,  4.1062e-01, -9.3079e-01, -1.6266e-01,\n",
              "           3.1995e-02]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn_encoder = TransformerEncoder(input_lang.n_words, 256)\n",
        "input = attn_encoder.tensorFromSentence(pairs[0][0])\n",
        "attn_encoder(input)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUCEEzEsvGMn"
      },
      "source": [
        "### The Decoder\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Und7tPLIvGMn"
      },
      "source": [
        "#### Attention Decoder\n",
        "\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of the encoder's outputs for every step of the decoder's own outputs.\n",
        "For this, we determine a set of attention weights which capture which part of the sentence is most important for the translation.\n",
        "The attention weights will be multiplied by the encoder output vectors to create a weighted combination.\n",
        "The result should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
        "\n",
        "Specifically, this is done as follows:\n",
        "1. Attention weights are calculated with a feed-forward layer (`self.attn`) using the concatenated decoder's input and hidden state as inputs and appying a softmax to it.\n",
        "   * Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n",
        "1. The attention weights will be multiplied by the encoder output vectors to create a weighted combination (using `torch.bmm()`).\n",
        "1. The encoder outputs weighted by the attention weights are concatenated (use `torch.cat`) with the decoder's embeddings and fed into another feed-forward layer (`self.attn_combine`).\n",
        "1. ReLu is applied on the output of the attention module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PwdF09yvGMo"
      },
      "outputs": [],
      "source": [
        "Image(filename=\"img/decoder.png\", height=900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7jQT5UPnvGMo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TODO: Task 4 (10 pt)\n",
        "Implement the attention layer as described above and depicted in the figure.\n",
        "\n",
        "Show the train progress (3-5 min) when using the decoder with attention on the translation task (see \"Pre-trained GRU seq2seq model\" section below).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, max_length):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # attention layer (feed-forward layer)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        # the next feed-forward layer combines the attention weights with the encoder outputs\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "    def forward(self, embedded, hidden, encoder_outputs):\n",
        "\n",
        "        # calculate attention weights with self.attn using the concatenated decoder's input and hidden state as inputs and applying a softmax to it\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "\n",
        "        # multiply attention weights with encoder outputs to get new \"weighted sum\" context vector (using torch.bmm())\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        # the encoder outputs weighted by the attention weights are concatenated (use torch.cat) with the decoder's embeddings and fed into another feed-forward layer (self.attn_combine)\n",
        "        output_enc = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "\n",
        "        # ReLu is applied on the output of the attention module\n",
        "        output = F.relu(self.attn_combine(output_enc).unsqueeze(0))\n",
        "\n",
        "        return output, attn_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8c9yw5kE5oif"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, rnn_type='GRU', rnn_n_layers=1, batch_size=1, max_length=MAX_WORDS+1, device=device):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.rnn_type = rnn_type\n",
        "        self.rnn_n_layers = rnn_n_layers\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)  # Adjust layer name to 'attn'\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)  # Adjust layer name to 'attn_combine'\n",
        "        self.rnn = getattr(nn, self.rnn_type)(self.hidden_size, self.hidden_size, self.rnn_n_layers)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        c0 = self.init_hidden()\n",
        "        # h0 = self.init_hidden() # reuse hidden state of LSTM/GRU cell or reinitialize?\n",
        "\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            attn_hidden = hidden[0] # only hidden, not cell state\n",
        "        else:\n",
        "            attn_hidden = hidden # GRU has no cell state\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], attn_hidden[0]), 1)))\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        output = self.out(output[0])\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        weights = torch.zeros(self.rnn_n_layers, self.batch_size, self.hidden_size, device=device)\n",
        "        return (weights, weights) if self.rnn_type == 'LSTM' else weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XTWdb-uvGMp"
      },
      "source": [
        "\n",
        "# Training\n",
        "\n",
        "### Preparing Training Data\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4bSmXByOvGMp"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8tTVtV7vGMq"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the `<SOS>` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but [when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "cRXbgFQnvGMq"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_WORDS+1):\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-gYAkcAvGMq"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0ei32eYlvGMr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai3KWafxvGMr"
      },
      "source": [
        "### Plotting results\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "UdWdrxmVvGMt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjaE0v19vGMr"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "XkR7FiesvGMr"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of7lko38vGMt"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "joGj-tUNvGMt"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_WORDS+1):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edCMUH-jvGMt"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "k2AM2KAOvGMu"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsYC2ekZvGMu"
      },
      "source": [
        "## Training and Evaluating\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. When training from scratch after about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        "To speed-up the training, we do not randomly initialize the weights, but reuse the weights from a pretrained model.\n",
        "\n",
        "Note: run this cells to show that your code is correctly functioning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "5-EsJsDZ6AWz"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size, rnn_type='GRU').to(device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, rnn_type='GRU', batch_size=1, dropout_p=0.1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "XvtS8nxr6CTH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/38/sb94bl795bv7klhlb8l7pj_00000gn/T/ipykernel_43001/3712431664.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn_weights = F.softmax(self.attn(torch.cat((embedded[0], attn_hidden[0]), 1)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0m 5s (- 0m 49s) (100 10%) 6.5359\n",
            "0m 11s (- 0m 45s) (200 20%) 5.6653\n",
            "0m 17s (- 0m 40s) (300 30%) 5.4375\n",
            "0m 23s (- 0m 35s) (400 40%) 5.4725\n",
            "0m 29s (- 0m 29s) (500 50%) 5.1284\n",
            "0m 35s (- 0m 23s) (600 60%) 5.3260\n",
            "0m 41s (- 0m 17s) (700 70%) 5.3801\n",
            "0m 48s (- 0m 12s) (800 80%) 5.1263\n",
            "0m 54s (- 0m 6s) (900 90%) 5.0754\n",
            "1m 1s (- 0m 0s) (1000 100%) 5.0443\n"
          ]
        }
      ],
      "source": [
        "trainIters(encoder, attn_decoder, 10**3, print_every=100)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "NdvKJjeybf3t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> you have to do this now .\n",
            "= du musst das jetzt machen .\n",
            "< er ist nicht ist nicht nicht . <EOS>\n",
            "\n",
            "> i study for hours every day .\n",
            "= ich studiere jeden tag stunden lang .\n",
            "< ich habe nicht nicht nicht . <EOS>\n",
            "\n",
            "> tom has to stop doing that right now .\n",
            "= tom muss sofort damit aufhoren .\n",
            "< tom ist nicht nicht nicht . <EOS>\n",
            "\n",
            "> tom likes it in boston .\n",
            "= tom gefallt es in boston .\n",
            "< tom ist nicht nicht nicht . <EOS>\n",
            "\n",
            "> thanks for coming .\n",
            "= danke fur s kommen .\n",
            "< er ist nicht nicht zu . <EOS>\n",
            "\n",
            "> i kissed my wife .\n",
            "= ich kusste meine frau .\n",
            "< ich habe nicht nicht . <EOS>\n",
            "\n",
            "> tom can t win .\n",
            "= tom kann nicht gewinnen .\n",
            "< tom ist nicht nicht nicht . <EOS>\n",
            "\n",
            "> tom blames mary for everything .\n",
            "= tom gibt mary fur alles die schuld .\n",
            "< tom ist nicht nicht nicht . <EOS>\n",
            "\n",
            "> the student raised his hand .\n",
            "= der student hob die hand .\n",
            "< er ist nicht ist nicht nicht . <EOS>\n",
            "\n",
            "> we seldom go to boston anymore .\n",
            "= wir gehen nur noch selten nach boston .\n",
            "< er ist nicht nicht viel . <EOS>\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/38/sb94bl795bv7klhlb8l7pj_00000gn/T/ipykernel_43001/3712431664.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn_weights = F.softmax(self.attn(torch.cat((embedded[0], attn_hidden[0]), 1)))\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder, attn_decoder)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96cuplb1vGMu"
      },
      "source": [
        "#### Pre-trained GRU seq2seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg8dG7-VvGMu"
      },
      "source": [
        "Load the state dict including the weights of a pre-trained model. If the initialization does not work, the architecture of your decoder is likely incorrect. Keep in mind that the two feed-forward layers of the Attention module have to be named `attn` and `attn_combine`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "-D4sQ1JhvGMu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EncoderRNN(\n",
              "  (embedding): Embedding(12982, 256)\n",
              "  (rnn): GRU(256, 256)\n",
              ")"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "encoder_file = os.path.join('data', 'pretrained_encoder2.pt')\n",
        "pretrained_encoder = EncoderRNN(input_lang.n_words, hidden_size, rnn_type='GRU').to(device)\n",
        "# pretrained_encoder.load_state_dict(torch.load(encoder_file), strict=False)\n",
        "# pretrained_encoder.eval()\n",
        "# the two commented lines above are not working because the pretrained encoder has a different vocabulary size than the one we are using; were part of the initial code\n",
        "\n",
        "desired_vocab_size = 12982\n",
        "if pretrained_encoder.embedding.weight.shape[0] != desired_vocab_size:\n",
        "    pretrained_encoder.embedding = nn.Embedding(desired_vocab_size, hidden_size)\n",
        "\n",
        "my_encoder = EncoderRNN(input_lang.n_words, hidden_size, rnn_type='GRU').to(device)\n",
        "my_encoder.load_state_dict(pretrained_encoder.state_dict(), strict=False)\n",
        "my_encoder.eval()\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "vQu7awkavGMu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AttnDecoderRNN(\n",
              "  (embedding): Embedding(25355, 256)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (attn): Linear(in_features=512, out_features=11, bias=True)\n",
              "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (rnn): GRU(256, 256)\n",
              "  (out): Linear(in_features=256, out_features=25355, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "decoder_file = os.path.join('data', 'pretrained_decoder2.pt')\n",
        "pretrained_attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, rnn_type='GRU', dropout_p=0.1).to(device)\n",
        "# pretrained_attn_decoder.load_state_dict(torch.load(decoder_file), strict=False)\n",
        "# pretrained_attn_decoder.eval()\n",
        "# the two commented lines above are not working because the pretrained decoder has a different vocabulary size than the one we are using; were part of the initial code\n",
        "\n",
        "desired_vocab_size = 25355\n",
        "if pretrained_attn_decoder.embedding.weight.shape[0] != desired_vocab_size:\n",
        "    pretrained_attn_decoder.embedding = nn.Embedding(desired_vocab_size, hidden_size)\n",
        "\n",
        "my_attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, rnn_type='GRU', dropout_p=0.1).to(device)\n",
        "my_attn_decoder.load_state_dict(pretrained_attn_decoder.state_dict(), strict=False)\n",
        "my_attn_decoder.eval()\n",
        "\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SyyCuL27vGMv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/38/sb94bl795bv7klhlb8l7pj_00000gn/T/ipykernel_43001/3712431664.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn_weights = F.softmax(self.attn(torch.cat((embedded[0], attn_hidden[0]), 1)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0m 5s (- 0m 52s) (100 10%) 6.7667\n",
            "0m 11s (- 0m 47s) (200 20%) 5.3738\n",
            "0m 19s (- 0m 44s) (300 30%) 5.6792\n",
            "0m 28s (- 0m 42s) (400 40%) 5.4303\n",
            "0m 35s (- 0m 35s) (500 50%) 5.2713\n",
            "0m 41s (- 0m 27s) (600 60%) 5.3032\n",
            "0m 48s (- 0m 20s) (700 70%) 5.5730\n",
            "0m 54s (- 0m 13s) (800 80%) 5.3596\n",
            "1m 1s (- 0m 6s) (900 90%) 5.4227\n",
            "1m 7s (- 0m 0s) (1000 100%) 5.2920\n"
          ]
        }
      ],
      "source": [
        "trainIters(pretrained_encoder, pretrained_attn_decoder, 10**3, print_every=100) # to be changed to 10**4 as provided initially; changed to 10**3 to reduce the time needed to run the code\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "3iQtHR5YvGMv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> i may still win .\n",
            "= vielleicht gewinne ich noch .\n",
            "< ich bin nicht . . <EOS>\n",
            "\n",
            "> tom owes his job to mary .\n",
            "= tom verdankt seine arbeit maria .\n",
            "< ich bin nicht tom . . <EOS>\n",
            "\n",
            "> the train is leaving soon .\n",
            "= der zug fahrt bald ab .\n",
            "< ich bin nicht . . . <EOS>\n",
            "\n",
            "> tom found a good job .\n",
            "= tom hat eine gute arbeit gefunden .\n",
            "< wir sind nicht . . <EOS>\n",
            "\n",
            "> i m very proud of it .\n",
            "= ich bin darauf sehr stolz .\n",
            "< ich bin nicht . . . <EOS>\n",
            "\n",
            "> i m glad you re still here .\n",
            "= ich freue mich dass du noch hier bist .\n",
            "< ich bin nicht tom . . <EOS>\n",
            "\n",
            "> i hope you re not serious .\n",
            "= ich hoffe du meinst das nicht ernst .\n",
            "< ich bin nicht . . <EOS>\n",
            "\n",
            "> she stopped picking daisies .\n",
            "= sie horte auf ganseblumchen zu pflucken .\n",
            "< ich bin nicht . . <EOS>\n",
            "\n",
            "> i m restless .\n",
            "= ich bin ruhelos .\n",
            "< ich bin nicht . <EOS>\n",
            "\n",
            "> i thought that you d be ready .\n",
            "= ich dachte du warest so weit .\n",
            "< ich bin nicht tom . . <EOS>\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/38/sb94bl795bv7klhlb8l7pj_00000gn/T/ipykernel_43001/3712431664.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn_weights = F.softmax(self.attn(torch.cat((embedded[0], attn_hidden[0]), 1)))\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(pretrained_encoder, pretrained_attn_decoder)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZlnoQBPvGMy"
      },
      "source": [
        "# Transfer learning\n",
        "## Fine tune out-of-the box encoder-decoder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2C9JKpsPvGMz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TODO: Task 5 (10 pt)\n",
        "\n",
        "Load a transformer for the hugging face library and fine-tune it to your problem.\n",
        "Show the training progress of the model and the training metrics. Briefly\n",
        "explain (3-4 sentences) the model choice and comment on the outcomes.\n",
        "\n",
        "Note: you don't have to use the above-defined methods for training.\n",
        "Splitting the dataset in train and test is welcomed, but not required for the task.\n",
        "\n",
        "\"\"\"\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_text = [pair[0] for pair in pairs]\n",
        "tokens = tokenizer(source_text, return_tensors=\"pt\", padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = tokenizer([pair[1] for pair in pairs], return_tensors=\"pt\", padding=True)[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 500)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(tokens[\"input_ids\"], tokens[\"attention_mask\"], labels)\n",
        "# smaller sample size\n",
        "train_dataset = torch.utils.data.Subset(train_dataset, range(0, 5000))\n",
        "train_loader = DataLoader(train_dataset, batch_size=4)\n",
        "val_dataset = torch.utils.data.Subset(train_dataset, range(len(train_dataset) - 500, len(train_dataset)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "len(train_dataset), len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Batch: 100/1250, Loss: 0.0924\n",
            "Epoch: 1, Batch: 200/1250, Loss: 0.1190\n",
            "Epoch: 1, Batch: 300/1250, Loss: 0.8114\n",
            "Epoch: 1, Batch: 400/1250, Loss: 0.6717\n",
            "Epoch: 1, Batch: 500/1250, Loss: 1.2040\n",
            "Epoch: 1, Batch: 600/1250, Loss: 0.3516\n",
            "Epoch: 1, Batch: 700/1250, Loss: 0.1530\n",
            "Epoch: 1, Batch: 800/1250, Loss: 0.4431\n",
            "Epoch: 1, Batch: 900/1250, Loss: 0.1834\n",
            "Epoch: 1, Batch: 1000/1250, Loss: 0.5279\n",
            "Epoch: 1, Batch: 1100/1250, Loss: 0.0600\n",
            "Epoch: 1, Batch: 1200/1250, Loss: 0.5920\n",
            "Epoch: 1, Average Loss: 0.0970\n",
            "Epoch: 2, Batch: 100/1250, Loss: 0.1403\n",
            "Epoch: 2, Batch: 200/1250, Loss: 0.0956\n",
            "Epoch: 2, Batch: 300/1250, Loss: 0.2718\n",
            "Epoch: 2, Batch: 400/1250, Loss: 0.3578\n",
            "Epoch: 2, Batch: 500/1250, Loss: 0.4648\n",
            "Epoch: 2, Batch: 600/1250, Loss: 0.1099\n",
            "Epoch: 2, Batch: 700/1250, Loss: 0.1014\n",
            "Epoch: 2, Batch: 800/1250, Loss: 0.2920\n",
            "Epoch: 2, Batch: 900/1250, Loss: 0.0800\n",
            "Epoch: 2, Batch: 1000/1250, Loss: 0.2682\n",
            "Epoch: 2, Batch: 1100/1250, Loss: 0.0202\n",
            "Epoch: 2, Batch: 1200/1250, Loss: 0.2810\n",
            "Epoch: 2, Average Loss: 0.0487\n",
            "Epoch: 3, Batch: 100/1250, Loss: 0.0990\n",
            "Epoch: 3, Batch: 200/1250, Loss: 0.0641\n",
            "Epoch: 3, Batch: 300/1250, Loss: 0.1365\n",
            "Epoch: 3, Batch: 400/1250, Loss: 0.1851\n",
            "Epoch: 3, Batch: 500/1250, Loss: 0.2082\n",
            "Epoch: 3, Batch: 600/1250, Loss: 0.0215\n",
            "Epoch: 3, Batch: 700/1250, Loss: 0.0616\n",
            "Epoch: 3, Batch: 800/1250, Loss: 0.1184\n",
            "Epoch: 3, Batch: 900/1250, Loss: 0.0445\n",
            "Epoch: 3, Batch: 1000/1250, Loss: 0.1482\n",
            "Epoch: 3, Batch: 1100/1250, Loss: 0.0195\n",
            "Epoch: 3, Batch: 1200/1250, Loss: 0.1494\n",
            "Epoch: 3, Average Loss: 0.0292\n",
            "Epoch: 4, Batch: 100/1250, Loss: 0.0505\n",
            "Epoch: 4, Batch: 200/1250, Loss: 0.0458\n",
            "Epoch: 4, Batch: 300/1250, Loss: 0.0704\n",
            "Epoch: 4, Batch: 400/1250, Loss: 0.1899\n",
            "Epoch: 4, Batch: 500/1250, Loss: 0.1664\n",
            "Epoch: 4, Batch: 600/1250, Loss: 0.0091\n",
            "Epoch: 4, Batch: 700/1250, Loss: 0.0652\n",
            "Epoch: 4, Batch: 800/1250, Loss: 0.1069\n",
            "Epoch: 4, Batch: 900/1250, Loss: 0.0254\n",
            "Epoch: 4, Batch: 1000/1250, Loss: 0.1069\n",
            "Epoch: 4, Batch: 1100/1250, Loss: 0.0243\n",
            "Epoch: 4, Batch: 1200/1250, Loss: 0.1437\n",
            "Epoch: 4, Average Loss: 0.0222\n",
            "Epoch: 5, Batch: 100/1250, Loss: 0.0587\n",
            "Epoch: 5, Batch: 200/1250, Loss: 0.0471\n",
            "Epoch: 5, Batch: 300/1250, Loss: 0.1228\n",
            "Epoch: 5, Batch: 400/1250, Loss: 0.1159\n",
            "Epoch: 5, Batch: 500/1250, Loss: 0.1481\n",
            "Epoch: 5, Batch: 600/1250, Loss: 0.0447\n",
            "Epoch: 5, Batch: 700/1250, Loss: 0.0891\n",
            "Epoch: 5, Batch: 800/1250, Loss: 0.0888\n",
            "Epoch: 5, Batch: 900/1250, Loss: 0.0267\n",
            "Epoch: 5, Batch: 1000/1250, Loss: 0.0701\n",
            "Epoch: 5, Batch: 1100/1250, Loss: 0.0112\n",
            "Epoch: 5, Batch: 1200/1250, Loss: 0.0634\n",
            "Epoch: 5, Average Loss: 0.0188\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "learning_rate = 5e-5\n",
        "accumulation_steps = 4\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "effective_training_steps = len(train_loader) * num_epochs // accumulation_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=effective_training_steps)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(train_loader, 1):\n",
        "        # Unpack the batch\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        if i % accumulation_steps == 0 or i == len(train_loader):\n",
        "            # Update model parameters after accumulating gradients\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 0:  # Print every 100 batches\n",
        "            print(f\"Epoch: {epoch + 1}, Batch: {i}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    average_loss = running_loss / len(train_loader) / accumulation_steps\n",
        "\n",
        "    print(f\"Epoch: {epoch + 1}, Average Loss: {average_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch: 1/125, Loss: 0.0645\n",
            "Batch: 2/125, Loss: 0.0914\n",
            "Batch: 3/125, Loss: 0.0755\n",
            "Batch: 4/125, Loss: 0.0421\n",
            "Batch: 5/125, Loss: 0.0466\n",
            "Batch: 6/125, Loss: 0.0855\n",
            "Batch: 7/125, Loss: 0.0827\n",
            "Batch: 8/125, Loss: 0.1541\n",
            "Batch: 9/125, Loss: 0.1214\n",
            "Batch: 10/125, Loss: 0.0374\n",
            "Batch: 11/125, Loss: 0.0199\n",
            "Batch: 12/125, Loss: 0.0226\n",
            "Batch: 13/125, Loss: 0.0838\n",
            "Batch: 14/125, Loss: 0.0449\n",
            "Batch: 15/125, Loss: 0.0021\n",
            "Batch: 16/125, Loss: 0.2519\n",
            "Batch: 17/125, Loss: 0.0034\n",
            "Batch: 18/125, Loss: 0.0201\n",
            "Batch: 19/125, Loss: 0.0208\n",
            "Batch: 20/125, Loss: 0.0852\n",
            "Batch: 21/125, Loss: 0.0066\n",
            "Batch: 22/125, Loss: 0.0504\n",
            "Batch: 23/125, Loss: 0.0028\n",
            "Batch: 24/125, Loss: 0.0475\n",
            "Batch: 25/125, Loss: 0.0395\n",
            "Batch: 26/125, Loss: 0.0467\n",
            "Batch: 27/125, Loss: 0.0062\n",
            "Batch: 28/125, Loss: 0.0209\n",
            "Batch: 29/125, Loss: 0.0228\n",
            "Batch: 30/125, Loss: 0.0201\n",
            "Batch: 31/125, Loss: 0.0095\n",
            "Batch: 32/125, Loss: 0.0374\n",
            "Batch: 33/125, Loss: 0.0422\n",
            "Batch: 34/125, Loss: 0.0388\n",
            "Batch: 35/125, Loss: 0.0423\n",
            "Batch: 36/125, Loss: 0.1031\n",
            "Batch: 37/125, Loss: 0.0783\n",
            "Batch: 38/125, Loss: 0.0497\n",
            "Batch: 39/125, Loss: 0.0409\n",
            "Batch: 40/125, Loss: 0.0679\n",
            "Batch: 41/125, Loss: 0.1270\n",
            "Batch: 42/125, Loss: 0.0406\n",
            "Batch: 43/125, Loss: 0.0407\n",
            "Batch: 44/125, Loss: 0.1157\n",
            "Batch: 45/125, Loss: 0.0687\n",
            "Batch: 46/125, Loss: 0.0779\n",
            "Batch: 47/125, Loss: 0.0596\n",
            "Batch: 48/125, Loss: 0.2992\n",
            "Batch: 49/125, Loss: 0.1056\n",
            "Batch: 50/125, Loss: 0.1785\n",
            "Batch: 51/125, Loss: 0.0842\n",
            "Batch: 52/125, Loss: 0.1112\n",
            "Batch: 53/125, Loss: 0.1445\n",
            "Batch: 54/125, Loss: 0.0516\n",
            "Batch: 55/125, Loss: 0.0710\n",
            "Batch: 56/125, Loss: 0.0934\n",
            "Batch: 57/125, Loss: 0.0854\n",
            "Batch: 58/125, Loss: 0.1251\n",
            "Batch: 59/125, Loss: 0.1068\n",
            "Batch: 60/125, Loss: 0.0955\n",
            "Batch: 61/125, Loss: 0.0717\n",
            "Batch: 62/125, Loss: 0.1618\n",
            "Batch: 63/125, Loss: 0.0617\n",
            "Batch: 64/125, Loss: 0.0254\n",
            "Batch: 65/125, Loss: 0.0622\n",
            "Batch: 66/125, Loss: 0.0307\n",
            "Batch: 67/125, Loss: 0.0487\n",
            "Batch: 68/125, Loss: 0.0734\n",
            "Batch: 69/125, Loss: 0.0712\n",
            "Batch: 70/125, Loss: 0.1344\n",
            "Batch: 71/125, Loss: 0.0748\n",
            "Batch: 72/125, Loss: 0.1105\n",
            "Batch: 73/125, Loss: 0.0404\n",
            "Batch: 74/125, Loss: 0.0703\n",
            "Batch: 75/125, Loss: 0.0557\n",
            "Batch: 76/125, Loss: 0.0202\n",
            "Batch: 77/125, Loss: 0.0315\n",
            "Batch: 78/125, Loss: 0.0684\n",
            "Batch: 79/125, Loss: 0.0386\n",
            "Batch: 80/125, Loss: 0.1255\n",
            "Batch: 81/125, Loss: 0.0496\n",
            "Batch: 82/125, Loss: 0.0899\n",
            "Batch: 83/125, Loss: 0.0458\n",
            "Batch: 84/125, Loss: 0.0500\n",
            "Batch: 85/125, Loss: 0.0079\n",
            "Batch: 86/125, Loss: 0.0529\n",
            "Batch: 87/125, Loss: 0.0426\n",
            "Batch: 88/125, Loss: 0.0789\n",
            "Batch: 89/125, Loss: 0.0625\n",
            "Batch: 90/125, Loss: 0.0537\n",
            "Batch: 91/125, Loss: 0.0896\n",
            "Batch: 92/125, Loss: 0.0866\n",
            "Batch: 93/125, Loss: 0.0382\n",
            "Batch: 94/125, Loss: 0.0433\n",
            "Batch: 95/125, Loss: 0.0147\n",
            "Batch: 96/125, Loss: 0.0486\n",
            "Batch: 97/125, Loss: 0.0106\n",
            "Batch: 98/125, Loss: 0.0756\n",
            "Batch: 99/125, Loss: 0.0400\n",
            "Batch: 100/125, Loss: 0.0434\n",
            "Batch: 101/125, Loss: 0.0743\n",
            "Batch: 102/125, Loss: 0.1258\n",
            "Batch: 103/125, Loss: 0.0338\n",
            "Batch: 104/125, Loss: 0.0663\n",
            "Batch: 105/125, Loss: 0.1061\n",
            "Batch: 106/125, Loss: 0.1136\n",
            "Batch: 107/125, Loss: 0.0743\n",
            "Batch: 108/125, Loss: 0.0557\n",
            "Batch: 109/125, Loss: 0.0216\n",
            "Batch: 110/125, Loss: 0.0514\n",
            "Batch: 111/125, Loss: 0.0102\n",
            "Batch: 112/125, Loss: 0.0202\n",
            "Batch: 113/125, Loss: 0.0198\n",
            "Batch: 114/125, Loss: 0.1082\n",
            "Batch: 115/125, Loss: 0.0282\n",
            "Batch: 116/125, Loss: 0.0081\n",
            "Batch: 117/125, Loss: 0.0307\n",
            "Batch: 118/125, Loss: 0.0224\n",
            "Batch: 119/125, Loss: 0.1244\n",
            "Batch: 120/125, Loss: 0.0764\n",
            "Batch: 121/125, Loss: 0.0467\n",
            "Batch: 122/125, Loss: 0.0225\n",
            "Batch: 123/125, Loss: 0.0289\n",
            "Batch: 124/125, Loss: 0.0266\n",
            "Batch: 125/125, Loss: 0.0529\n",
            "Average Validation Loss: 0.0637\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "total_val_loss = 0.0\n",
        "num_val_batches = len(val_loader)\n",
        "\n",
        "for i, batch in enumerate(val_loader, 1):\n",
        "    # Unpack the batch\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "    total_val_loss += loss.item()\n",
        "\n",
        "    print(f\"Batch: {i}/{len(val_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "average_val_loss = total_val_loss / num_val_batches\n",
        "\n",
        "print(f\"Average Validation Loss: {average_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> tom has red hair and freckles .\n",
            "= tom hat rotes haar und sommersprossen .\n",
            "< tom hat rotes hair und freckles.\n",
            "\n",
            "> i had hoped we might become friends .\n",
            "= ich hatte gehofft wir konnten freunde werden .\n",
            "< ich habe mich erhofft wir konnen freund gehen.\n",
            "\n",
            "> i want to consult you about something .\n",
            "= ich hatte gerne deinen rat in einer sache .\n",
            "< ich mochte dich zu etwas beruhigen.\n",
            "\n",
            "> tom says that he s tired .\n",
            "= tom sagt dass er mude ist .\n",
            "< tom sagt er ist mude.\n",
            "\n",
            "> i ve never found that to be necessary .\n",
            "= ich habe das nie fur notig befunden .\n",
            "< ich habe das nie für notwendig gefunden.\n",
            "\n",
            "> i think he s a great writer .\n",
            "= ich finde er ist ein toller schriftsteller .\n",
            "< ich stimme zu, er ist ein grossartiger Schriftstellerin.\n",
            "\n",
            "> tom always made me laugh .\n",
            "= tom brachte mich immer zum lachen .\n",
            "< tom hat mich immer zum lacheln gekommen.\n",
            "\n",
            "> tom has few friends .\n",
            "= tom hat wenige freunde .\n",
            "< tom hat noch ein sein freund.\n",
            "\n",
            "> he denied the accusation .\n",
            "= er verneinte die anklage .\n",
            "< er bestritt die Anschuldigung.\n",
            "\n",
            "> your honesty is refreshing .\n",
            "= deine ehrlichkeit ist wohltuend .\n",
            "< ihr ehrlichkeit ist erhollich.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model randomly\n",
        "\n",
        "def evaluateRandomly(model, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = model.generate(**tokenizer(pair[0], return_tensors=\"pt\", padding=True))\n",
        "        output_sentence = tokenizer.decode(output_words[0], skip_special_tokens=True)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "evaluateRandomly(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEXE_Wa8vGMM"
      },
      "source": [
        "# Credits\n",
        "\n",
        "This problem set is based upon an official PyTorch [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). Many thanks to PyTorch, [Sean Robertson](https://github.com/spro/practical-pytorch) and  [Florian Nachtigall](https://github.com/FlorianNachtigall).\n",
        "\n",
        "Be cautious with looking in the original notebook for answers. Many details have been changed and you won't be able to copy-and-paste solutions.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "6718ee69a5bb56082d0947ea42dc2c4709b07299f2a8d77179593d74021d8684"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
